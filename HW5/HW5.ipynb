{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">EE 380L: Data Mining</p>\n",
    "# <p style=\"text-align: center;\">Assignment 5</p>\n",
    "## <p style=\"text-align: center;\">Total points: 40</p>\n",
    "## <p style=\"text-align: center;\">Due: Thursday, April 24th, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Xiaocheng Shen\n",
    "## UT EID: xs2948\n",
    "\n",
    "## Name: Dawei Liang\n",
    "## UT EID: dl33629"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Random Forest vs Boosting - Regression (15pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will compare performance of different ensemble methods for regression problems: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), [Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) (GBR), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor). We will use the Appliances energy prediction dataset for this problem https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction. Use random_state = 42 for the algorithms.\n",
    "\n",
    "1. Use a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Find the best parameters (*n_estimators*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report the accuracy of your model in terms of RMSE. (5pts)\n",
    "\n",
    "2. Use [Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) (GBR). Find the best parameters (including *n_estimators,* *max_depth* and* learning_rate*), and report corresponding RMSE for each algorithm. (5pts)\n",
    "\n",
    "3. Use [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor) for predicting the targets. Find the best parameters (*n_estimators*, *learning_rate*). Report the accuracy of your model in terms of RMSE. (5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import (RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13222, 56) (13222,) (6513, 56) (6513,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('energydata_complete.csv') \n",
    "\n",
    "y = data['Appliances']\n",
    "X = data.drop(['date','Appliances', 'rv1', 'rv2'], axis = 1)\n",
    "\n",
    "\n",
    "from dateutil import parser\n",
    "timeData = np.array(data['date'])\n",
    "\n",
    "days = []\n",
    "hours = []\n",
    "for line in range(len(timeData)):\n",
    "    day = parser.parse(timeData[line]).weekday()\n",
    "    hour = parser.parse(timeData[line]).hour\n",
    "    days.append(day)\n",
    "    hours.append(hour)\n",
    "    \n",
    "X = pd.concat([X, pd.get_dummies(days), pd.get_dummies(hours)], axis = 1)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=42)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: {'n_estimators': 50}\n",
      "RF:rmse, 71.63264911664761\n"
     ]
    }
   ],
   "source": [
    "parameters = {'n_estimators':[1,5,10,20,50]}\n",
    "rf = RandomForestRegressor(random_state=42,)\n",
    "rg = GridSearchCV(rf, parameters)\n",
    "model = rg.fit(x_train, y_train)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, model.predict(x_test)))\n",
    "print('RF:',model.best_params_ )\n",
    "print('RF:rmse,',rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT: {'n_estimators': 200} best learning rate: 0.5 best depth: 5\n",
      "GBDT:rmse 75.8764397161184\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.01,0.05,0.1,0.2,0.5]\n",
    "max_depth = [1,2,3,4,5]\n",
    "rmse_list = [0] * 25\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        rg2 = GradientBoostingRegressor(learning_rate=learning_rate[j], max_depth=max_depth[i])\n",
    "        rg2.fit(x_train, y_train)\n",
    "        rmse_list[i * 5 + j] = np.sqrt(mean_squared_error(y_test, rg2.predict(x_test)))\n",
    "\n",
    "best_i = rmse_list.index(min(rmse_list))\n",
    "learning_rate, max_depth = learning_rate[best_i%5], max_depth[int(best_i/5)]\n",
    "\n",
    "parameters = {'n_estimators':[10,50,100,200,300]}\n",
    "rf2 = GradientBoostingRegressor(learning_rate=learning_rate, max_depth=max_depth)\n",
    "rg2 = GridSearchCV(rf2, parameters)\n",
    "model2 = rg2.fit(x_train, y_train)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, model2.predict(x_test)))\n",
    "print('GBDT:', model2.best_params_, 'best learning rate:',learning_rate, 'best depth:',max_depth)\n",
    "print('GBDT:rmse',rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost: {'n_estimators': 10} best learning rate: 0.1\n",
      "Adaboost:rmse, 96.81017955674213\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.1,0.5,1,2,5]\n",
    "rmse_list = [0] * 5\n",
    "\n",
    "for i in range(5):\n",
    "    rg3 = AdaBoostRegressor(learning_rate=learning_rate[i])\n",
    "    rg3.fit(x_train, y_train)\n",
    "    score_list[i] = np.sqrt(mean_squared_error(y_test, rg3.predict(x_test)))\n",
    "\n",
    "best_i = rmse_list.index(min(rmse_list))\n",
    "learning_rate= learning_rate[best_i%5]\n",
    "\n",
    "parameters = {'n_estimators':[1,10,50,100,200]}\n",
    "rf3 = AdaBoostRegressor(learning_rate=learning_rate)\n",
    "rg3 = GridSearchCV(rf3, parameters)\n",
    "model3 = rg3.fit(x_train, y_train)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, model3.predict(x_test)))\n",
    "print('Adaboost:',model3.best_params_,'best learning rate:',learning_rate)\n",
    "print('Adaboost:rmse,',rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Random Forest vs Boosting - Classification (15 pts)\n",
    "In this question, we will compare performance of different ensemble methods for classification problems: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier). As in HW4, you will be classifying signal vs background in the MAGIC Gamma Telescope Data Set. The data has been split into training and test as well as standardized for you.\n",
    "\n",
    "1. Use a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for the classification. Set the random_state to 42. Find the best parameters (by varying *n_estimators* and *criterion*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report your testing accuracy ([accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)) and [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score). You will need [predict_proba](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba) for roc_auc_score. (4pts)\n",
    "\n",
    "2. Use [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) for the spam classification problem. Again, find the best parameters (including *n_estimators, learning_rate,* and *max_depth (GBDT only)*), and report corresponding accuracy_score and roc_auc_score on the test data for each algorithm. (8pts)\n",
    "\n",
    "3. Point out one advantage and one disadvantage of Random Forest compared to GBDT (3pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score)\n",
    "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaochengshen/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# train\n",
    "train = pd.read_csv('magic_train.csv', header=None)\n",
    "y_train = train.values[:,10]\n",
    "y_train[y_train == 'g'] = 0\n",
    "y_train[y_train == 'h'] = 1\n",
    "y_train = y_train.astype(float)\n",
    "x_train = train.values[:,:10]\n",
    "\n",
    "# test\n",
    "test = pd.read_csv('magic_test.csv', header=None)\n",
    "y_test = test.values[:,10]\n",
    "y_test[y_test == 'g'] = 0\n",
    "y_test[y_test == 'h'] = 1\n",
    "y_test = y_test.astype(float)\n",
    "x_test = test.values[:,:10]\n",
    "\n",
    "# standardize the data\n",
    "x_train = preprocessing.scale(x_train)\n",
    "x_test = preprocessing.scale(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: {'criterion': 'gini', 'n_estimators': 200}\n",
      "RF:accuracy_score, 0.8768318213538032\n",
      "RF:roc_auc_score, 0.9321763568768894\n"
     ]
    }
   ],
   "source": [
    "parameters = {'n_estimators':[1,10,50,100,200], 'criterion':['gini','entropy']}\n",
    "rf = RandomForestClassifier(random_state=42,)\n",
    "clf = GridSearchCV(rf, parameters)\n",
    "model = clf.fit(x_train, y_train)\n",
    "print('RF:',model.best_params_ )\n",
    "print('RF:accuracy_score,',accuracy_score(y_test, model.predict(x_test)))\n",
    "print('RF:roc_auc_score,',roc_auc_score(y_test, model.predict_proba(x_test)[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT: {'n_estimators': 200} best learning rate: 0.01 best depth: 1\n",
      "GBDT:accuracy_score 0.7418004187020237\n",
      "GBDT:roc_auc_score 0.8381682548439772\n",
      "Adaboost: {'n_estimators': 200} best learning rate: 0.1\n",
      "Adaboost:accuracy_score, 0.8473482205163991\n",
      "Adaboost:roc_auc_score 0.9000435473322483\n"
     ]
    }
   ],
   "source": [
    "learning_rate = [0.01,0.05,0.1,0.2,0.5]\n",
    "max_depth = [1,2,3,4,5]\n",
    "score_list = [0] * 25\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        clf2 = GradientBoostingClassifier(learning_rate=learning_rate[j], max_depth=max_depth[i])\n",
    "        clf2.fit(x_train, y_train)\n",
    "        score_list[i * 5 + j] = clf.score(x_test, y_test)\n",
    "\n",
    "best_i = score_list.index(max(score_list))\n",
    "learning_rate, max_depth = learning_rate[best_i%5], max_depth[int(best_i/5)]\n",
    "\n",
    "parameters = {'n_estimators':[1,10,50,100,200]}\n",
    "rf2 = GradientBoostingClassifier(learning_rate=learning_rate, max_depth=max_depth)\n",
    "clf2 = GridSearchCV(rf2, parameters)\n",
    "model2 = clf2.fit(x_train, y_train)\n",
    "print('GBDT:', model2.best_params_, 'best learning rate:',learning_rate, 'best depth:',max_depth)\n",
    "print('GBDT:accuracy_score',accuracy_score(y_test, model2.predict(x_test)))\n",
    "print('GBDT:roc_auc_score',roc_auc_score(y_test, model2.predict_proba(x_test)[:,1]))\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = [0.1,0.5,1,2,5]\n",
    "score_list = [0] * 5\n",
    "\n",
    "for i in range(5):\n",
    "    clf3 = GradientBoostingClassifier(learning_rate=learning_rate[i])\n",
    "    clf3.fit(x_train, y_train)\n",
    "    score_list[i] = clf.score(x_test, y_test)\n",
    "\n",
    "best_i = score_list.index(max(score_list))\n",
    "learning_rate= learning_rate[best_i%5]\n",
    "\n",
    "parameters = {'n_estimators':[1,10,50,100,200]}\n",
    "rf3 = AdaBoostClassifier()\n",
    "clf3 = GridSearchCV(rf3, parameters)\n",
    "model3 = clf3.fit(x_train, y_train)\n",
    "print('Adaboost:',model3.best_params_,'best learning rate:',learning_rate)\n",
    "print('Adaboost:accuracy_score,',accuracy_score(y_test, model3.predict(x_test)))\n",
    "print('Adaboost:roc_auc_score',roc_auc_score(y_test, model3.predict_proba(x_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GBDT is based on weak learners which are shallow trees, sometimes the trees are as small as with two leaves. GBDT reduces error mainly by reducing bias.\n",
    "\n",
    "\n",
    "Random Forest uses fully grown decision trees (low bias, high variance). It reduces error by reducing variance. The trees are made uncorrelated to maximize the decrease in variance without reducing bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Visualization using Bokeh (10 pts)\n",
    "\n",
    "In this problem, you'll build an interactive visualization. Bokeh is a Python interactive visualization library that targets modern web browsers for presentation. For more information on Bokeh, see http://bokeh.pydata.org/en/latest/. The problem statement is as follows:\n",
    "\n",
    "Using the \"nbasalaries_new.csv\" data set, your goal is to build a Bokeh visualization which allows the user to explore how salary (on a log scale) varies with points per game (PSG) and age. You will create a visualization that allows the user to toggle the X axis of a scatter plot between PSG and age, with the y-axis always being log Salary. Also add the hover tool so that if the user hovers over a datapoint in the plot a window pops up that shows the player name, team, position, salary, and the current x variable (PSG or age) depending on the current tab.  Add the ability to Zoom in/out.\n",
    "\n",
    "Hints: \n",
    "1. see: http://bokeh.pydata.org/en/latest/docs/user_guide/tools.html#basic-tooltips for hover and zoom tool examples.\n",
    "2. See: http://bokeh.pydata.org/en/latest/docs/reference/plotting.html. Look for the scatter API.\n",
    "3. See: http://bokeh.pydata.org/en/0.10.0/docs/user_guide/styling.html#labels. For labeling axes.\n",
    "4. See: https://bokeh.pydata.org/en/latest/docs/user_guide/categorical.html  for how to use jitter transform\n",
    "5. Use output_file() from Bokeh to output the plot to an external file. Submit this file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bokeh.models.widgets import Panel, Tabs\n",
    "from bokeh.models import CustomJS, ColumnDataSource, HoverTool, BoxZoomTool, Jitter\n",
    "from bokeh.plotting import figure, output_notebook, show, output_file\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.models.widgets import Toggle\n",
    "\n",
    "data = pd.read_csv(\"nbasalaries_new.csv\")\n",
    "data[\"logsalary\"] = data.SALARY.apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
